{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4802e26a-c6a8-453f-a972-6a284ad1bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e1a454-4991-44e9-b134-6a40a3691465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42 for all relevant libraries.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os # To set environment variables, useful for some libraries\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the random seed for reproducibility across different libraries.\n",
    "    \"\"\"\n",
    "    # 1. Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 2. Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 3. Set seed for PyTorch (CPU and GPU)\n",
    "    torch.manual_seed(seed) # For CPU operations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed) # For current GPU\n",
    "        torch.cuda.manual_seed_all(seed) # For all GPUs (if you have multiple)\n",
    "\n",
    "    # 4. Ensure deterministic behavior for CuDNN (GPU operations)\n",
    "    #    This can sometimes slightly slow down training, but ensures exact reproducibility.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # Disable CuDNN auto-tuner for deterministic ops\n",
    "\n",
    "    # 5. Set environment variable for Python hashing (affects dicts, sets, etc.)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    print(f\"Random seed set to {seed} for all relevant libraries.\")\n",
    "\n",
    "MY_RANDOM_SEED = 42\n",
    "set_seed(MY_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0449044-053f-43eb-b028-95a33c430d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  width  height  class  \\\n",
      "0  egohands-public-1620914960773_png_jpg.rf.aa184...    640     640   Rock   \n",
      "1  egohands-public-1624053434391_png_jpg.rf.aaef5...    640     640  Paper   \n",
      "2  egohands-public-1624465902684_png_jpg.rf.aaa09...    640     640   Rock   \n",
      "3  Screen-Shot-2022-02-08-at-12-59-24-PM_png.rf.a...    640     640   Rock   \n",
      "4  egohands-public-1622127402076_png_jpg.rf.aa897...    640     640   Rock   \n",
      "\n",
      "   xmin  ymin  xmax  ymax  \n",
      "0   429   185   562   319  \n",
      "1   269   354   544   443  \n",
      "2   427   332   551   509  \n",
      "3    80   268   145   395  \n",
      "4    83   128   296   381  \n",
      "egohands-public-1620914960773_png_jpg.rf.aa184eeebad98b2fb04354d01a90b9d0.jpg\n",
      "0\n",
      "class\n",
      "Paper       1349\n",
      "Rock        1924\n",
      "Scissors    1337\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_csv_file_path = 'C:/Users/elbru/Projects_DL/Rock_Paper_Scissors/rock-paper-scissors/train/train/_annotations.csv'\n",
    "train_raw_df = pd.read_csv(train_csv_file_path)\n",
    "\n",
    "print(train_raw_df.head())\n",
    "print(train_raw_df.filename[0])\n",
    "print(train_raw_df.loc[:,'class'].isnull().sum())#So all the images are labelled\n",
    "\n",
    "print(train_raw_df.groupby('class').size()) #So all images correctly belong to either Papper, Rock, Scissors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99b7bb5-19ea-4bd2-8bc4-7226b10e6777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4610 entries, 0 to 4609\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  4610 non-null   object\n",
      " 1   class     4610 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 72.2+ KB\n",
      "None\n",
      "filename    egohands-public-1620914960773_png_jpg.rf.aa184...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_df = train_raw_df.loc[:, ['filename','class']]\n",
    "print(train_df.info())\n",
    "print(train_df.loc[0,['filename']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5cdff5-444c-48b8-9bf4-38db2f074f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204 entries, 0 to 203\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  204 non-null    object\n",
      " 1   width     204 non-null    int64 \n",
      " 2   height    204 non-null    int64 \n",
      " 3   class     204 non-null    object\n",
      " 4   xmin      204 non-null    int64 \n",
      " 5   ymin      204 non-null    int64 \n",
      " 6   xmax      204 non-null    int64 \n",
      " 7   ymax      204 non-null    int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 12.9+ KB\n",
      "None\n",
      "0\n",
      "class\n",
      "Paper       72\n",
      "Rock        65\n",
      "Scissors    67\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "evaluate_csv_file_path ='rock-paper-scissors/test/test/_annotations.csv'\n",
    "\n",
    "evaluate_raw_df = pd.read_csv(evaluate_csv_file_path)\n",
    "\n",
    "print(evaluate_raw_df.info())\n",
    "print(evaluate_raw_df.loc[:,'class'].isnull().sum())#So all the images are labelled\n",
    "\n",
    "print(evaluate_raw_df.groupby('class').size()) #So all images correctly belong to either Papper, Rock, Scissors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4d8643-890b-489e-9560-0151a724db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204 entries, 0 to 203\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  204 non-null    object\n",
      " 1   class     204 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.3+ KB\n",
      "None\n",
      "filename    IMG_7079_MOV-23_jpg.rf.123a8de8c8da646e4a25f1c...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "evaluate_df = evaluate_raw_df.loc[:, ['filename','class']]\n",
    "print(evaluate_df.info())\n",
    "print(evaluate_df.loc[0,['filename']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00800216-164a-470d-bf95-423169c4cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01adaa4b-2224-4a40-a5f7-067f7a457bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# --- Image dimensions for thr model ---\n",
    "IMAGE_SIZE = 16\n",
    "\n",
    "\n",
    "# --- 1. Define Transformations (same as before, but applied within custom dataset) ---\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), #for our model to work, it's mandatory to have same size images\n",
    "    transforms.RandomHorizontalFlip(),#a way to artificially increase the dataset, and improve generaliztion\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06764348-4213-4850-91a8-00df9b0e7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "#Datasets\n",
    "\n",
    "class RockPaperScissorsDataset(Dataset):\n",
    "\n",
    "    def __init__(self,df, img_file_path, transform):\n",
    "        self.df = df\n",
    "        self.img_file_path = img_file_path\n",
    "        self.transform = transform\n",
    "        self.class_to_value = {'Rock': 0, 'Paper': 1, 'Scissors': 2}\n",
    "        self.list_images_tensors =[]\n",
    "        for idx in range(len(df)):\n",
    "            # Open the image\n",
    "            image = Image.open(self.img_file_path + self.df.loc[idx,'filename'])\n",
    "            # Ensure it's RGB (important for consistency)\n",
    "            image = image.convert('RGB')\n",
    "            image_tensor = self.transform(image)\n",
    "            image.close()\n",
    "            self.list_images_tensors.append(image_tensor)\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image_tensor = self.list_images_tensors[idx]\n",
    "        ground_truth= self.class_to_value[self.df.loc[idx,'class']]\n",
    "\n",
    "        return image_tensor,ground_truth\n",
    "train_dataset= RockPaperScissorsDataset(train_df,'C:/Users/elbru/Projects_DL/Rock_Paper_Scissors/rock-paper-scissors/train/train/',train_transforms)\n",
    "\n",
    "evaluate_dataset = RockPaperScissorsDataset(evaluate_df, 'rock-paper-scissors/test/test/', test_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4841e6a-3ec4-4730-8289-a7bf8f73410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8706, 0.8824, 0.8902, 0.8627, 0.8627, 0.9059, 0.6549, 0.4510,\n",
      "          0.4863, 0.4471, 0.6980, 0.7098, 0.6706, 0.6471, 0.6275, 0.6275],\n",
      "         [0.8980, 0.9294, 0.9412, 0.9059, 0.8784, 0.8510, 0.5686, 0.5843,\n",
      "          0.5765, 0.4863, 0.5529, 0.6549, 0.6784, 0.7059, 0.6980, 0.7137],\n",
      "         [0.8667, 0.8824, 0.9529, 0.9569, 0.8745, 0.6824, 0.5490, 0.5529,\n",
      "          0.5333, 0.5176, 0.5608, 0.7608, 0.7765, 0.7882, 0.7843, 0.7725],\n",
      "         [0.8353, 0.7451, 0.8471, 0.9098, 0.8627, 0.6824, 0.5961, 0.5804,\n",
      "          0.5647, 0.5804, 0.5686, 0.7490, 0.7373, 0.7725, 0.8078, 0.8314],\n",
      "         [0.8902, 0.8118, 0.8314, 0.8039, 0.6863, 0.6353, 0.6314, 0.6471,\n",
      "          0.6157, 0.6745, 0.6431, 0.9373, 0.9843, 1.0000, 1.0000, 1.0000],\n",
      "         [0.8392, 0.7882, 0.8431, 0.8745, 0.7373, 0.6510, 0.5882, 0.6588,\n",
      "          0.6196, 0.6745, 0.6902, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9333, 1.0000, 1.0000, 0.8706, 0.7490, 0.5137, 0.6784,\n",
      "          0.6549, 0.6314, 0.6941, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9333, 1.0000, 1.0000, 0.8824, 0.7725, 0.4588, 0.6706,\n",
      "          0.6667, 0.5961, 0.6392, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9490, 1.0000, 0.9294, 0.7882, 0.6824, 0.5255, 0.6745,\n",
      "          0.6627, 0.6510, 0.6314, 0.9804, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9412, 0.8824, 0.7020, 0.6510, 0.5451, 0.5255, 0.6471,\n",
      "          0.6745, 0.6627, 0.6431, 0.8078, 0.9961, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9569, 0.8392, 0.6275, 0.6078, 0.5216, 0.5137, 0.6471,\n",
      "          0.6471, 0.6039, 0.6275, 0.6902, 0.8588, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9490, 0.8000, 0.6510, 0.6039, 0.4980, 0.5333, 0.6353,\n",
      "          0.5882, 0.5294, 0.5961, 0.6510, 0.7804, 0.9961, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9255, 0.7765, 0.6275, 0.5216, 0.4588, 0.4588, 0.5373,\n",
      "          0.4941, 0.5255, 0.5765, 0.6275, 0.7216, 0.9373, 1.0000, 1.0000],\n",
      "         [0.8039, 0.7647, 0.6863, 0.5529, 0.4627, 0.4549, 0.4392, 0.4392,\n",
      "          0.4471, 0.4941, 0.5412, 0.5961, 0.6824, 0.8196, 0.9647, 0.9765],\n",
      "         [0.7451, 0.7137, 0.6275, 0.5098, 0.4588, 0.4588, 0.4549, 0.4431,\n",
      "          0.4549, 0.4980, 0.5294, 0.5647, 0.6471, 0.6980, 0.7216, 0.6863],\n",
      "         [0.6824, 0.6588, 0.5765, 0.4392, 0.4353, 0.4353, 0.4392, 0.4431,\n",
      "          0.4627, 0.4784, 0.5020, 0.5294, 0.6078, 0.6471, 0.6275, 0.6157]],\n",
      "\n",
      "        [[0.9137, 0.9216, 0.9137, 0.8784, 0.8784, 0.9216, 0.6510, 0.3725,\n",
      "          0.3804, 0.4157, 0.7216, 0.7529, 0.7176, 0.6784, 0.6471, 0.6471],\n",
      "         [0.9569, 0.9765, 0.9686, 0.9137, 0.8824, 0.8549, 0.5373, 0.4588,\n",
      "          0.4314, 0.3961, 0.5412, 0.6588, 0.7020, 0.7333, 0.7333, 0.7373],\n",
      "         [0.9373, 0.9451, 0.9961, 0.9686, 0.8784, 0.6824, 0.4980, 0.4431,\n",
      "          0.4118, 0.4196, 0.5373, 0.7804, 0.8000, 0.8118, 0.8196, 0.8039],\n",
      "         [0.8902, 0.7922, 0.8863, 0.9373, 0.8745, 0.6745, 0.5176, 0.4588,\n",
      "          0.4275, 0.4627, 0.5216, 0.7529, 0.7608, 0.7922, 0.8275, 0.8471],\n",
      "         [0.9451, 0.8549, 0.8824, 0.8471, 0.7137, 0.6196, 0.5216, 0.4824,\n",
      "          0.4510, 0.5176, 0.5882, 0.9333, 0.9922, 1.0000, 1.0000, 1.0000],\n",
      "         [0.8863, 0.8157, 0.8824, 0.8902, 0.7451, 0.6431, 0.5059, 0.5020,\n",
      "          0.4549, 0.5451, 0.6549, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9569, 1.0000, 1.0000, 0.8745, 0.7373, 0.4549, 0.5333,\n",
      "          0.4902, 0.5294, 0.6745, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9686, 1.0000, 1.0000, 0.8824, 0.7451, 0.4078, 0.5412,\n",
      "          0.5176, 0.5176, 0.6275, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9765, 1.0000, 0.9255, 0.7373, 0.5765, 0.4000, 0.5412,\n",
      "          0.5216, 0.5451, 0.5765, 0.9647, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9725, 0.8784, 0.6118, 0.5137, 0.3647, 0.3529, 0.4902,\n",
      "          0.5255, 0.5294, 0.5176, 0.7412, 0.9961, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9882, 0.7882, 0.4980, 0.4627, 0.3294, 0.3255, 0.4588,\n",
      "          0.4784, 0.4392, 0.4588, 0.5882, 0.8196, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9765, 0.7451, 0.5216, 0.4431, 0.2863, 0.3412, 0.4588,\n",
      "          0.3961, 0.3529, 0.4196, 0.5373, 0.7137, 0.9922, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9529, 0.7137, 0.4863, 0.3294, 0.2510, 0.2863, 0.3569,\n",
      "          0.3176, 0.3333, 0.3843, 0.4941, 0.6431, 0.9059, 1.0000, 1.0000],\n",
      "         [0.9216, 0.8196, 0.5961, 0.4078, 0.2588, 0.2431, 0.2431, 0.2627,\n",
      "          0.2784, 0.3098, 0.3333, 0.4510, 0.5922, 0.7843, 0.9686, 0.9804],\n",
      "         [0.8549, 0.7176, 0.5216, 0.3529, 0.2471, 0.2392, 0.2471, 0.2588,\n",
      "          0.2667, 0.2902, 0.3137, 0.4118, 0.5451, 0.6314, 0.6941, 0.6863],\n",
      "         [0.7686, 0.6235, 0.4549, 0.3098, 0.2314, 0.2392, 0.2431, 0.2510,\n",
      "          0.2627, 0.2863, 0.2902, 0.3725, 0.5059, 0.5765, 0.6039, 0.6157]],\n",
      "\n",
      "        [[0.8824, 0.8784, 0.8627, 0.8314, 0.8314, 0.8784, 0.6157, 0.3725,\n",
      "          0.3765, 0.3922, 0.6824, 0.7176, 0.6824, 0.6549, 0.6275, 0.6235],\n",
      "         [0.9294, 0.9412, 0.9294, 0.8706, 0.8314, 0.8118, 0.5098, 0.4471,\n",
      "          0.4196, 0.3843, 0.5098, 0.6392, 0.6784, 0.7098, 0.7098, 0.7098],\n",
      "         [0.9098, 0.9059, 0.9608, 0.9294, 0.8314, 0.6431, 0.4863, 0.4392,\n",
      "          0.4078, 0.4118, 0.5059, 0.7569, 0.7843, 0.8039, 0.8118, 0.8000],\n",
      "         [0.8706, 0.7765, 0.8706, 0.9098, 0.8392, 0.6392, 0.5020, 0.4549,\n",
      "          0.4275, 0.4510, 0.5059, 0.7451, 0.7608, 0.8039, 0.8275, 0.8510],\n",
      "         [0.9255, 0.8431, 0.8706, 0.8275, 0.7059, 0.5922, 0.5020, 0.4824,\n",
      "          0.4510, 0.4980, 0.5804, 0.9333, 0.9922, 1.0000, 1.0000, 1.0000],\n",
      "         [0.8902, 0.8275, 0.8902, 0.8941, 0.7451, 0.6157, 0.4902, 0.5020,\n",
      "          0.4588, 0.5294, 0.6510, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9686, 1.0000, 1.0000, 0.8745, 0.7137, 0.4471, 0.5294,\n",
      "          0.4902, 0.5216, 0.6706, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9804, 1.0000, 1.0000, 0.8784, 0.7216, 0.3961, 0.5333,\n",
      "          0.5020, 0.5020, 0.6314, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 1.0000, 0.9255, 0.7255, 0.5843, 0.4078, 0.5216,\n",
      "          0.4980, 0.5294, 0.5922, 0.9647, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9882, 0.8824, 0.6039, 0.5098, 0.4039, 0.3647, 0.4549,\n",
      "          0.4941, 0.5137, 0.5569, 0.7569, 0.9961, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9961, 0.7843, 0.4980, 0.4588, 0.3686, 0.3490, 0.4314,\n",
      "          0.4549, 0.4510, 0.5137, 0.6039, 0.8039, 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9922, 0.7412, 0.5137, 0.4431, 0.3373, 0.3569, 0.4314,\n",
      "          0.4000, 0.4039, 0.4824, 0.5529, 0.6941, 0.9882, 1.0000, 1.0000],\n",
      "         [1.0000, 0.9686, 0.6980, 0.4745, 0.3569, 0.3059, 0.3176, 0.3569,\n",
      "          0.3490, 0.3843, 0.4471, 0.5137, 0.6196, 0.8902, 1.0000, 1.0000],\n",
      "         [0.9725, 0.8431, 0.5843, 0.4118, 0.3137, 0.2902, 0.3020, 0.3059,\n",
      "          0.3137, 0.3608, 0.4039, 0.4706, 0.5686, 0.7569, 0.9686, 0.9804],\n",
      "         [0.9098, 0.7216, 0.4980, 0.3608, 0.3059, 0.2941, 0.3020, 0.3020,\n",
      "          0.3176, 0.3490, 0.3765, 0.4431, 0.5255, 0.6118, 0.6902, 0.6902],\n",
      "         [0.8275, 0.6157, 0.4471, 0.3255, 0.2863, 0.2824, 0.2902, 0.3020,\n",
      "          0.3137, 0.3255, 0.3529, 0.4078, 0.4863, 0.5490, 0.5843, 0.6078]]]) 0\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[2][0], train_dataset[2][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4471df22-68dc-494d-8b1b-85f78a88b618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 16, 16])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "#let's make the dataloaders \n",
    "training_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "evaluate_dataloader = DataLoader(evaluate_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for X,y in training_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a038-9fdd-45e2-8c10-b2b9e7ee5a89",
   "metadata": {},
   "source": [
    "'''nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels//2, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(hidden_channels//2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d51cf45e-3e23-4215-8d84-7deb6a188422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RockPaperScissorsCNN(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_size, images_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels,hidden_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1) ),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels * images_size[0]//2 * images_size[1]//2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.cnn(x)\n",
    "        return logits \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b9f4af8-c710-4c59-bf06-5ec85cbb14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels):\n",
    "        super().__init__(),\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(input_channels,hidden_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.downsample = None\n",
    "        if input_channels != hidden_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, hidden_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_channels)\n",
    "            )\n",
    "            #this way we make sure that F(x) + x is doable as x and F(x)are the same size\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.convs(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return output + x # residual connection\n",
    "\n",
    "class RockPaperScissorsResNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_size, images_size, dropout_prob):\n",
    "        super().__init__()     \n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ResBlock(input_channels, hidden_channels),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(hidden_channels, hidden_channels),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels * images_size[0]//4 * images_size[1]//4, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        logits = self.net(X)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7abe982d-da0d-4d4a-9cae-96fbd9c6739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 3\n",
    "hidden_channels = 128\n",
    "output_size = 3\n",
    "images_size = (16,16)\n",
    "dropout_prob = 0.3\n",
    "\n",
    "model_CNN = RockPaperScissorsCNN(input_channels, hidden_channels, output_size, images_size, dropout_prob).to(device)\n",
    "criterion_CNN = nn.CrossEntropyLoss() # We use CrossEntropyLoss as we are solving a classification problem\n",
    "optimizer_CNN = torch.optim.AdamW(model_CNN.parameters(), lr=0.0001, weight_decay=0.0001) \n",
    "\n",
    "model_NET = RockPaperScissorsResNetwork(input_channels, hidden_channels, output_size, images_size, dropout_prob).to(device)\n",
    "criterion_NET = nn.CrossEntropyLoss() # We use CrossEntropyLoss as we are solving a classification problem\n",
    "optimizer_NET = torch.optim.AdamW(model_NET.parameters(), lr=0.0001, weight_decay=0.0001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e1e0408-158f-4ab1-89e8-1213b332e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader,model,criterion, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_samples_processed_in_epoch = 0 #Initialize a variable to track the total samples processed in this epoch\n",
    "    model.train()\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred=model(X)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() #update of weights and biases  \n",
    "        optimizer.zero_grad() #gradient reset \n",
    "\n",
    "        #Accumulate the number of samples processed in the current batch\n",
    "        total_samples_processed_in_epoch += len(X) \n",
    "\n",
    "\n",
    "        if batch%10==0:\n",
    "            loss_val = loss.item()\n",
    "            print(f\"loss: {loss_val:>7f}  [{total_samples_processed_in_epoch:>5d}/{size:>5d}]\")\n",
    "\n",
    "def evaluate_loop(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    elements_per_batch = len(dataloader)\n",
    "    model.eval()\n",
    "    sum_loss_per_batch, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            sum_loss_per_batch+=criterion(pred,y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    sum_loss_per_batch/=elements_per_batch\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_loss_per_batch:>8f} \\n\")\n",
    "    \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44baf629-5246-40f1-b03c-64b67d7e60e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.236581  [  128/ 4610]\n",
      "loss: 1.112476  [ 1408/ 4610]\n",
      "loss: 1.148637  [ 2688/ 4610]\n",
      "loss: 1.143614  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.090566 \n",
      "\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.073129  [  128/ 4610]\n",
      "loss: 1.108783  [ 1408/ 4610]\n",
      "loss: 1.073652  [ 2688/ 4610]\n",
      "loss: 1.093443  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 40.7%, Avg loss: 1.085266 \n",
      "\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.122612  [  128/ 4610]\n",
      "loss: 1.017558  [ 1408/ 4610]\n",
      "loss: 1.151183  [ 2688/ 4610]\n",
      "loss: 1.008859  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 1.098568 \n",
      "\n",
      "Current Learning Rate: 0.000099\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.027956  [  128/ 4610]\n",
      "loss: 1.129416  [ 1408/ 4610]\n",
      "loss: 1.077867  [ 2688/ 4610]\n",
      "loss: 1.095463  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 1.094681 \n",
      "\n",
      "Current Learning Rate: 0.000099\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.090351  [  128/ 4610]\n",
      "loss: 1.141703  [ 1408/ 4610]\n",
      "loss: 1.007635  [ 2688/ 4610]\n",
      "loss: 0.981297  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 43.1%, Avg loss: 1.046055 \n",
      "\n",
      "Current Learning Rate: 0.000098\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.056868  [  128/ 4610]\n",
      "loss: 0.970874  [ 1408/ 4610]\n",
      "loss: 1.010353  [ 2688/ 4610]\n",
      "loss: 1.084014  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 46.1%, Avg loss: 1.060521 \n",
      "\n",
      "Current Learning Rate: 0.000097\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.043466  [  128/ 4610]\n",
      "loss: 1.131320  [ 1408/ 4610]\n",
      "loss: 1.062535  [ 2688/ 4610]\n",
      "loss: 0.987745  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.058834 \n",
      "\n",
      "Current Learning Rate: 0.000096\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.005429  [  128/ 4610]\n",
      "loss: 1.044044  [ 1408/ 4610]\n",
      "loss: 1.011804  [ 2688/ 4610]\n",
      "loss: 0.970067  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.045144 \n",
      "\n",
      "Current Learning Rate: 0.000094\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.016054  [  128/ 4610]\n",
      "loss: 1.032585  [ 1408/ 4610]\n",
      "loss: 1.002689  [ 2688/ 4610]\n",
      "loss: 1.043100  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 1.048855 \n",
      "\n",
      "Current Learning Rate: 0.000093\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.021551  [  128/ 4610]\n",
      "loss: 0.991129  [ 1408/ 4610]\n",
      "loss: 1.141706  [ 2688/ 4610]\n",
      "loss: 1.053346  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.063327 \n",
      "\n",
      "Current Learning Rate: 0.000091\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.951794  [  128/ 4610]\n",
      "loss: 0.967552  [ 1408/ 4610]\n",
      "loss: 1.011202  [ 2688/ 4610]\n",
      "loss: 1.063034  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.034385 \n",
      "\n",
      "Current Learning Rate: 0.000090\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.013193  [  128/ 4610]\n",
      "loss: 0.960488  [ 1408/ 4610]\n",
      "loss: 1.092750  [ 2688/ 4610]\n",
      "loss: 1.043809  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.074911 \n",
      "\n",
      "Current Learning Rate: 0.000088\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.923033  [  128/ 4610]\n",
      "loss: 0.997346  [ 1408/ 4610]\n",
      "loss: 0.990999  [ 2688/ 4610]\n",
      "loss: 0.984933  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.038477 \n",
      "\n",
      "Current Learning Rate: 0.000086\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.938378  [  128/ 4610]\n",
      "loss: 0.953983  [ 1408/ 4610]\n",
      "loss: 1.005931  [ 2688/ 4610]\n",
      "loss: 1.013055  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.026245 \n",
      "\n",
      "Current Learning Rate: 0.000084\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.990474  [  128/ 4610]\n",
      "loss: 1.067640  [ 1408/ 4610]\n",
      "loss: 1.035144  [ 2688/ 4610]\n",
      "loss: 1.044730  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.024273 \n",
      "\n",
      "Current Learning Rate: 0.000081\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.027680  [  128/ 4610]\n",
      "loss: 0.910176  [ 1408/ 4610]\n",
      "loss: 1.042181  [ 2688/ 4610]\n",
      "loss: 0.975637  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.055362 \n",
      "\n",
      "Current Learning Rate: 0.000079\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.981514  [  128/ 4610]\n",
      "loss: 0.966148  [ 1408/ 4610]\n",
      "loss: 1.004366  [ 2688/ 4610]\n",
      "loss: 1.070713  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.056759 \n",
      "\n",
      "Current Learning Rate: 0.000077\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.016181  [  128/ 4610]\n",
      "loss: 0.993076  [ 1408/ 4610]\n",
      "loss: 0.999169  [ 2688/ 4610]\n",
      "loss: 1.000414  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.021911 \n",
      "\n",
      "Current Learning Rate: 0.000074\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.908146  [  128/ 4610]\n",
      "loss: 0.922272  [ 1408/ 4610]\n",
      "loss: 0.992022  [ 2688/ 4610]\n",
      "loss: 1.020883  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.039495 \n",
      "\n",
      "Current Learning Rate: 0.000072\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.068913  [  128/ 4610]\n",
      "loss: 0.989327  [ 1408/ 4610]\n",
      "loss: 0.919898  [ 2688/ 4610]\n",
      "loss: 1.000611  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.018516 \n",
      "\n",
      "Current Learning Rate: 0.000069\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.919017  [  128/ 4610]\n",
      "loss: 0.964216  [ 1408/ 4610]\n",
      "loss: 0.923927  [ 2688/ 4610]\n",
      "loss: 0.922750  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.019596 \n",
      "\n",
      "Current Learning Rate: 0.000066\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.943196  [  128/ 4610]\n",
      "loss: 0.956594  [ 1408/ 4610]\n",
      "loss: 0.914787  [ 2688/ 4610]\n",
      "loss: 0.902596  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.023282 \n",
      "\n",
      "Current Learning Rate: 0.000063\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.936280  [  128/ 4610]\n",
      "loss: 1.018266  [ 1408/ 4610]\n",
      "loss: 0.981426  [ 2688/ 4610]\n",
      "loss: 1.012057  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.018459 \n",
      "\n",
      "Current Learning Rate: 0.000061\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.008473  [  128/ 4610]\n",
      "loss: 1.047755  [ 1408/ 4610]\n",
      "loss: 0.934454  [ 2688/ 4610]\n",
      "loss: 0.974567  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 46.6%, Avg loss: 1.019974 \n",
      "\n",
      "Current Learning Rate: 0.000058\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.897197  [  128/ 4610]\n",
      "loss: 0.991824  [ 1408/ 4610]\n",
      "loss: 0.936503  [ 2688/ 4610]\n",
      "loss: 1.065273  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.023085 \n",
      "\n",
      "Current Learning Rate: 0.000055\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.886304  [  128/ 4610]\n",
      "loss: 1.023355  [ 1408/ 4610]\n",
      "loss: 0.949974  [ 2688/ 4610]\n",
      "loss: 0.921340  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.016469 \n",
      "\n",
      "Current Learning Rate: 0.000052\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.972196  [  128/ 4610]\n",
      "loss: 0.940840  [ 1408/ 4610]\n",
      "loss: 0.987271  [ 2688/ 4610]\n",
      "loss: 0.934971  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.026009 \n",
      "\n",
      "Current Learning Rate: 0.000049\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.991235  [  128/ 4610]\n",
      "loss: 0.970582  [ 1408/ 4610]\n",
      "loss: 0.944122  [ 2688/ 4610]\n",
      "loss: 0.972069  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.014434 \n",
      "\n",
      "Current Learning Rate: 0.000047\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.912333  [  128/ 4610]\n",
      "loss: 0.914518  [ 1408/ 4610]\n",
      "loss: 0.939188  [ 2688/ 4610]\n",
      "loss: 0.962052  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.024095 \n",
      "\n",
      "Current Learning Rate: 0.000044\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.900846  [  128/ 4610]\n",
      "loss: 0.976592  [ 1408/ 4610]\n",
      "loss: 0.981038  [ 2688/ 4610]\n",
      "loss: 0.958165  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.025598 \n",
      "\n",
      "Current Learning Rate: 0.000041\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.891820  [  128/ 4610]\n",
      "loss: 0.968730  [ 1408/ 4610]\n",
      "loss: 0.883832  [ 2688/ 4610]\n",
      "loss: 0.994524  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.010579 \n",
      "\n",
      "Current Learning Rate: 0.000038\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.920521  [  128/ 4610]\n",
      "loss: 0.980377  [ 1408/ 4610]\n",
      "loss: 0.915852  [ 2688/ 4610]\n",
      "loss: 0.993884  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 1.021189 \n",
      "\n",
      "Current Learning Rate: 0.000036\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.879908  [  128/ 4610]\n",
      "loss: 0.965677  [ 1408/ 4610]\n",
      "loss: 0.951891  [ 2688/ 4610]\n",
      "loss: 0.935743  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.010297 \n",
      "\n",
      "Current Learning Rate: 0.000033\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.931686  [  128/ 4610]\n",
      "loss: 0.976764  [ 1408/ 4610]\n",
      "loss: 0.998955  [ 2688/ 4610]\n",
      "loss: 0.932445  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.009820 \n",
      "\n",
      "Current Learning Rate: 0.000031\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.004969  [  128/ 4610]\n",
      "loss: 0.925383  [ 1408/ 4610]\n",
      "loss: 0.955062  [ 2688/ 4610]\n",
      "loss: 1.015062  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 1.010142 \n",
      "\n",
      "Current Learning Rate: 0.000029\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.868829  [  128/ 4610]\n",
      "loss: 0.977945  [ 1408/ 4610]\n",
      "loss: 1.048900  [ 2688/ 4610]\n",
      "loss: 0.975967  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.008232 \n",
      "\n",
      "Current Learning Rate: 0.000026\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.036264  [  128/ 4610]\n",
      "loss: 0.999548  [ 1408/ 4610]\n",
      "loss: 0.929385  [ 2688/ 4610]\n",
      "loss: 0.944313  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.017449 \n",
      "\n",
      "Current Learning Rate: 0.000024\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.957384  [  128/ 4610]\n",
      "loss: 0.951012  [ 1408/ 4610]\n",
      "loss: 0.890869  [ 2688/ 4610]\n",
      "loss: 0.943868  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.004529 \n",
      "\n",
      "Current Learning Rate: 0.000022\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.900928  [  128/ 4610]\n",
      "loss: 0.920123  [ 1408/ 4610]\n",
      "loss: 0.923131  [ 2688/ 4610]\n",
      "loss: 0.886182  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.005171 \n",
      "\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.932961  [  128/ 4610]\n",
      "loss: 0.930273  [ 1408/ 4610]\n",
      "loss: 0.990425  [ 2688/ 4610]\n",
      "loss: 0.952096  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.002098 \n",
      "\n",
      "Current Learning Rate: 0.000019\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.951416  [  128/ 4610]\n",
      "loss: 0.989840  [ 1408/ 4610]\n",
      "loss: 0.897417  [ 2688/ 4610]\n",
      "loss: 0.903899  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.003732 \n",
      "\n",
      "Current Learning Rate: 0.000017\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.971751  [  128/ 4610]\n",
      "loss: 0.912028  [ 1408/ 4610]\n",
      "loss: 0.908223  [ 2688/ 4610]\n",
      "loss: 0.950905  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.003468 \n",
      "\n",
      "Current Learning Rate: 0.000016\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.960549  [  128/ 4610]\n",
      "loss: 0.925395  [ 1408/ 4610]\n",
      "loss: 0.934564  [ 2688/ 4610]\n",
      "loss: 0.897418  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.012934 \n",
      "\n",
      "Current Learning Rate: 0.000014\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.875869  [  128/ 4610]\n",
      "loss: 1.027140  [ 1408/ 4610]\n",
      "loss: 0.947950  [ 2688/ 4610]\n",
      "loss: 0.965323  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.004345 \n",
      "\n",
      "Current Learning Rate: 0.000013\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.936857  [  128/ 4610]\n",
      "loss: 0.994755  [ 1408/ 4610]\n",
      "loss: 0.937353  [ 2688/ 4610]\n",
      "loss: 0.942049  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.016406 \n",
      "\n",
      "Current Learning Rate: 0.000012\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.909850  [  128/ 4610]\n",
      "loss: 0.978080  [ 1408/ 4610]\n",
      "loss: 0.886201  [ 2688/ 4610]\n",
      "loss: 0.971428  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 1.006875 \n",
      "\n",
      "Current Learning Rate: 0.000011\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.960118  [  128/ 4610]\n",
      "loss: 0.964231  [ 1408/ 4610]\n",
      "loss: 0.959450  [ 2688/ 4610]\n",
      "loss: 0.939991  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.003405 \n",
      "\n",
      "Current Learning Rate: 0.000011\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.899912  [  128/ 4610]\n",
      "loss: 0.984133  [ 1408/ 4610]\n",
      "loss: 0.941317  [ 2688/ 4610]\n",
      "loss: 0.862559  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.003078 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.901498  [  128/ 4610]\n",
      "loss: 0.881160  [ 1408/ 4610]\n",
      "loss: 0.877626  [ 2688/ 4610]\n",
      "loss: 0.984648  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 1.001278 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.942289  [  128/ 4610]\n",
      "loss: 0.929290  [ 1408/ 4610]\n",
      "loss: 0.935745  [ 2688/ 4610]\n",
      "loss: 1.026572  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.002969 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "scheduler_CNN = lr_scheduler.CosineAnnealingLR(optimizer_CNN, T_max=epochs, eta_min=1e-5)\n",
    "for iteration in range(epochs):\n",
    "    print(f\"Epoch {iteration+1}\\n-------------------------------\")\n",
    "    training_loop(training_dataloader,model_CNN,criterion_CNN,optimizer_CNN)\n",
    "    evaluate_loop(evaluate_dataloader,model_CNN,criterion_CNN)\n",
    "    scheduler_CNN.step()\n",
    "    #display the current learning rate \n",
    "    current_lr = optimizer_CNN.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "946ff97f-88b3-4a33-8fe9-11ebacf538be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.284534  [  128/ 4610]\n",
      "loss: 1.219621  [ 1408/ 4610]\n",
      "loss: 1.139328  [ 2688/ 4610]\n",
      "loss: 1.138654  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 1.105445 \n",
      "\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.175048  [  128/ 4610]\n",
      "loss: 1.148569  [ 1408/ 4610]\n",
      "loss: 1.053559  [ 2688/ 4610]\n",
      "loss: 1.064710  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 1.072272 \n",
      "\n",
      "Current Learning Rate: 0.000100\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.040254  [  128/ 4610]\n",
      "loss: 0.971461  [ 1408/ 4610]\n",
      "loss: 1.130785  [ 2688/ 4610]\n",
      "loss: 1.024841  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 38.2%, Avg loss: 1.076205 \n",
      "\n",
      "Current Learning Rate: 0.000099\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.034228  [  128/ 4610]\n",
      "loss: 1.216150  [ 1408/ 4610]\n",
      "loss: 1.067649  [ 2688/ 4610]\n",
      "loss: 1.039423  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.095316 \n",
      "\n",
      "Current Learning Rate: 0.000099\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.053615  [  128/ 4610]\n",
      "loss: 1.061173  [ 1408/ 4610]\n",
      "loss: 1.105421  [ 2688/ 4610]\n",
      "loss: 0.925012  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.5%, Avg loss: 1.035258 \n",
      "\n",
      "Current Learning Rate: 0.000098\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.069288  [  128/ 4610]\n",
      "loss: 0.988689  [ 1408/ 4610]\n",
      "loss: 0.983014  [ 2688/ 4610]\n",
      "loss: 1.017731  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 1.043620 \n",
      "\n",
      "Current Learning Rate: 0.000097\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.046628  [  128/ 4610]\n",
      "loss: 0.946890  [ 1408/ 4610]\n",
      "loss: 0.971785  [ 2688/ 4610]\n",
      "loss: 1.029295  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.1%, Avg loss: 1.035363 \n",
      "\n",
      "Current Learning Rate: 0.000096\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.044292  [  128/ 4610]\n",
      "loss: 1.064930  [ 1408/ 4610]\n",
      "loss: 1.156007  [ 2688/ 4610]\n",
      "loss: 1.031516  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 48.0%, Avg loss: 1.047962 \n",
      "\n",
      "Current Learning Rate: 0.000094\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.025186  [  128/ 4610]\n",
      "loss: 0.910085  [ 1408/ 4610]\n",
      "loss: 1.000162  [ 2688/ 4610]\n",
      "loss: 1.037126  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.022238 \n",
      "\n",
      "Current Learning Rate: 0.000093\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.005853  [  128/ 4610]\n",
      "loss: 1.092227  [ 1408/ 4610]\n",
      "loss: 1.040335  [ 2688/ 4610]\n",
      "loss: 0.966699  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 1.013811 \n",
      "\n",
      "Current Learning Rate: 0.000091\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.982308  [  128/ 4610]\n",
      "loss: 0.966256  [ 1408/ 4610]\n",
      "loss: 1.000943  [ 2688/ 4610]\n",
      "loss: 0.959727  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 1.062732 \n",
      "\n",
      "Current Learning Rate: 0.000090\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.967557  [  128/ 4610]\n",
      "loss: 1.023223  [ 1408/ 4610]\n",
      "loss: 1.046074  [ 2688/ 4610]\n",
      "loss: 0.984239  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.016392 \n",
      "\n",
      "Current Learning Rate: 0.000088\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.932405  [  128/ 4610]\n",
      "loss: 0.946295  [ 1408/ 4610]\n",
      "loss: 0.863550  [ 2688/ 4610]\n",
      "loss: 0.915691  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 47.5%, Avg loss: 1.017089 \n",
      "\n",
      "Current Learning Rate: 0.000086\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.988208  [  128/ 4610]\n",
      "loss: 0.912400  [ 1408/ 4610]\n",
      "loss: 1.018766  [ 2688/ 4610]\n",
      "loss: 0.972068  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.002857 \n",
      "\n",
      "Current Learning Rate: 0.000084\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.980381  [  128/ 4610]\n",
      "loss: 1.041194  [ 1408/ 4610]\n",
      "loss: 0.975142  [ 2688/ 4610]\n",
      "loss: 0.979728  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 1.052372 \n",
      "\n",
      "Current Learning Rate: 0.000081\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.941171  [  128/ 4610]\n",
      "loss: 0.943066  [ 1408/ 4610]\n",
      "loss: 0.953044  [ 2688/ 4610]\n",
      "loss: 0.965981  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 1.014730 \n",
      "\n",
      "Current Learning Rate: 0.000079\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.887623  [  128/ 4610]\n",
      "loss: 1.002822  [ 1408/ 4610]\n",
      "loss: 0.916478  [ 2688/ 4610]\n",
      "loss: 0.992071  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 1.014269 \n",
      "\n",
      "Current Learning Rate: 0.000077\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.946208  [  128/ 4610]\n",
      "loss: 0.826194  [ 1408/ 4610]\n",
      "loss: 0.896602  [ 2688/ 4610]\n",
      "loss: 0.919627  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.029269 \n",
      "\n",
      "Current Learning Rate: 0.000074\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.834053  [  128/ 4610]\n",
      "loss: 0.920541  [ 1408/ 4610]\n",
      "loss: 0.830429  [ 2688/ 4610]\n",
      "loss: 0.849753  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.990943 \n",
      "\n",
      "Current Learning Rate: 0.000072\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.929308  [  128/ 4610]\n",
      "loss: 0.951183  [ 1408/ 4610]\n",
      "loss: 0.815532  [ 2688/ 4610]\n",
      "loss: 0.978582  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 53.9%, Avg loss: 0.973260 \n",
      "\n",
      "Current Learning Rate: 0.000069\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.915084  [  128/ 4610]\n",
      "loss: 0.921987  [ 1408/ 4610]\n",
      "loss: 0.863198  [ 2688/ 4610]\n",
      "loss: 0.934652  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 1.019591 \n",
      "\n",
      "Current Learning Rate: 0.000066\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.937964  [  128/ 4610]\n",
      "loss: 0.855472  [ 1408/ 4610]\n",
      "loss: 0.952714  [ 2688/ 4610]\n",
      "loss: 0.963201  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 1.012170 \n",
      "\n",
      "Current Learning Rate: 0.000063\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.856389  [  128/ 4610]\n",
      "loss: 0.910189  [ 1408/ 4610]\n",
      "loss: 0.910718  [ 2688/ 4610]\n",
      "loss: 0.850468  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 0.996206 \n",
      "\n",
      "Current Learning Rate: 0.000061\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.878288  [  128/ 4610]\n",
      "loss: 0.829690  [ 1408/ 4610]\n",
      "loss: 0.889202  [ 2688/ 4610]\n",
      "loss: 0.934229  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.992058 \n",
      "\n",
      "Current Learning Rate: 0.000058\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.926831  [  128/ 4610]\n",
      "loss: 0.962981  [ 1408/ 4610]\n",
      "loss: 0.803152  [ 2688/ 4610]\n",
      "loss: 0.874481  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.971060 \n",
      "\n",
      "Current Learning Rate: 0.000055\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.812817  [  128/ 4610]\n",
      "loss: 0.911508  [ 1408/ 4610]\n",
      "loss: 0.924456  [ 2688/ 4610]\n",
      "loss: 0.857476  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.956686 \n",
      "\n",
      "Current Learning Rate: 0.000052\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.934630  [  128/ 4610]\n",
      "loss: 0.891289  [ 1408/ 4610]\n",
      "loss: 0.954723  [ 2688/ 4610]\n",
      "loss: 0.766925  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.996071 \n",
      "\n",
      "Current Learning Rate: 0.000049\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.883073  [  128/ 4610]\n",
      "loss: 0.868717  [ 1408/ 4610]\n",
      "loss: 0.926298  [ 2688/ 4610]\n",
      "loss: 0.872336  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.969009 \n",
      "\n",
      "Current Learning Rate: 0.000047\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.906021  [  128/ 4610]\n",
      "loss: 0.837386  [ 1408/ 4610]\n",
      "loss: 0.893052  [ 2688/ 4610]\n",
      "loss: 0.938004  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.970670 \n",
      "\n",
      "Current Learning Rate: 0.000044\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.913519  [  128/ 4610]\n",
      "loss: 0.813000  [ 1408/ 4610]\n",
      "loss: 0.980002  [ 2688/ 4610]\n",
      "loss: 0.876352  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.978805 \n",
      "\n",
      "Current Learning Rate: 0.000041\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.849838  [  128/ 4610]\n",
      "loss: 0.835244  [ 1408/ 4610]\n",
      "loss: 0.897273  [ 2688/ 4610]\n",
      "loss: 0.813774  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.964971 \n",
      "\n",
      "Current Learning Rate: 0.000038\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.858760  [  128/ 4610]\n",
      "loss: 0.875218  [ 1408/ 4610]\n",
      "loss: 0.856305  [ 2688/ 4610]\n",
      "loss: 0.892487  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.966493 \n",
      "\n",
      "Current Learning Rate: 0.000036\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.844035  [  128/ 4610]\n",
      "loss: 0.822003  [ 1408/ 4610]\n",
      "loss: 0.934078  [ 2688/ 4610]\n",
      "loss: 0.859509  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Avg loss: 0.984692 \n",
      "\n",
      "Current Learning Rate: 0.000033\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.816426  [  128/ 4610]\n",
      "loss: 0.804190  [ 1408/ 4610]\n",
      "loss: 0.820717  [ 2688/ 4610]\n",
      "loss: 0.883352  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.973896 \n",
      "\n",
      "Current Learning Rate: 0.000031\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.910023  [  128/ 4610]\n",
      "loss: 0.826732  [ 1408/ 4610]\n",
      "loss: 0.837264  [ 2688/ 4610]\n",
      "loss: 0.839646  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.973090 \n",
      "\n",
      "Current Learning Rate: 0.000029\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.847686  [  128/ 4610]\n",
      "loss: 0.900490  [ 1408/ 4610]\n",
      "loss: 0.840390  [ 2688/ 4610]\n",
      "loss: 0.888048  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.947645 \n",
      "\n",
      "Current Learning Rate: 0.000026\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.791681  [  128/ 4610]\n",
      "loss: 0.815178  [ 1408/ 4610]\n",
      "loss: 0.827556  [ 2688/ 4610]\n",
      "loss: 0.817245  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.946302 \n",
      "\n",
      "Current Learning Rate: 0.000024\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.809953  [  128/ 4610]\n",
      "loss: 0.876498  [ 1408/ 4610]\n",
      "loss: 0.812404  [ 2688/ 4610]\n",
      "loss: 0.786088  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.955933 \n",
      "\n",
      "Current Learning Rate: 0.000022\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.888756  [  128/ 4610]\n",
      "loss: 0.773533  [ 1408/ 4610]\n",
      "loss: 0.858243  [ 2688/ 4610]\n",
      "loss: 0.855328  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.955873 \n",
      "\n",
      "Current Learning Rate: 0.000020\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.861048  [  128/ 4610]\n",
      "loss: 0.957380  [ 1408/ 4610]\n",
      "loss: 0.897683  [ 2688/ 4610]\n",
      "loss: 0.866462  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.951263 \n",
      "\n",
      "Current Learning Rate: 0.000019\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.892999  [  128/ 4610]\n",
      "loss: 0.807173  [ 1408/ 4610]\n",
      "loss: 0.807782  [ 2688/ 4610]\n",
      "loss: 0.909820  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.960024 \n",
      "\n",
      "Current Learning Rate: 0.000017\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.808535  [  128/ 4610]\n",
      "loss: 0.864343  [ 1408/ 4610]\n",
      "loss: 0.815528  [ 2688/ 4610]\n",
      "loss: 0.829852  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.955671 \n",
      "\n",
      "Current Learning Rate: 0.000016\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.817255  [  128/ 4610]\n",
      "loss: 0.776097  [ 1408/ 4610]\n",
      "loss: 0.792595  [ 2688/ 4610]\n",
      "loss: 0.839307  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.959256 \n",
      "\n",
      "Current Learning Rate: 0.000014\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.795082  [  128/ 4610]\n",
      "loss: 0.855637  [ 1408/ 4610]\n",
      "loss: 0.821715  [ 2688/ 4610]\n",
      "loss: 0.830608  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.949580 \n",
      "\n",
      "Current Learning Rate: 0.000013\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.861044  [  128/ 4610]\n",
      "loss: 0.746231  [ 1408/ 4610]\n",
      "loss: 0.808518  [ 2688/ 4610]\n",
      "loss: 0.795177  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 0.948250 \n",
      "\n",
      "Current Learning Rate: 0.000012\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.811078  [  128/ 4610]\n",
      "loss: 0.825482  [ 1408/ 4610]\n",
      "loss: 0.861229  [ 2688/ 4610]\n",
      "loss: 0.848496  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 0.948811 \n",
      "\n",
      "Current Learning Rate: 0.000011\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.765417  [  128/ 4610]\n",
      "loss: 0.827877  [ 1408/ 4610]\n",
      "loss: 0.794947  [ 2688/ 4610]\n",
      "loss: 0.814353  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.957928 \n",
      "\n",
      "Current Learning Rate: 0.000011\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.889523  [  128/ 4610]\n",
      "loss: 0.852480  [ 1408/ 4610]\n",
      "loss: 0.732223  [ 2688/ 4610]\n",
      "loss: 0.843481  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.946389 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.800289  [  128/ 4610]\n",
      "loss: 0.844760  [ 1408/ 4610]\n",
      "loss: 0.808075  [ 2688/ 4610]\n",
      "loss: 0.804602  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.957952 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.876636  [  128/ 4610]\n",
      "loss: 0.870285  [ 1408/ 4610]\n",
      "loss: 0.804756  [ 2688/ 4610]\n",
      "loss: 0.763861  [ 3968/ 4610]\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Avg loss: 0.945775 \n",
      "\n",
      "Current Learning Rate: 0.000010\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "scheduler_NET = lr_scheduler.CosineAnnealingLR(optimizer_NET, T_max=epochs, eta_min=1e-5)\n",
    "for iteration in range(epochs):\n",
    "    print(f\"Epoch {iteration+1}\\n-------------------------------\")\n",
    "    training_loop(training_dataloader,model_NET,criterion_NET,optimizer_NET)\n",
    "    evaluate_loop(evaluate_dataloader,model_NET,criterion_NET)\n",
    "    scheduler_NET.step()\n",
    "    #display the current learning rate \n",
    "    current_lr = optimizer_NET.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
